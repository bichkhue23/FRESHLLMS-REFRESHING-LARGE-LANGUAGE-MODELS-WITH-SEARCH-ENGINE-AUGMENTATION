{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cIMEmj9Esik",
        "outputId": "0b6f45b2-3211-4174-8115-6e3ac4811917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.8-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.8\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2023.11.17)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=c56f214ff3910cea954406a0c803b1fb9c42d6b015f680bd17de1a5b6c3d0366\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ],
      "source": [
        "#@title Installing required Python packages\n",
        "!pip install openai\n",
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wW3P9qyxHHU8"
      },
      "outputs": [],
      "source": [
        "#@title Importing Python libraries and modules\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import pytz\n",
        "import dateutil\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "from openai import OpenAI\n",
        "import tabulate\n",
        "import textwrap\n",
        "\n",
        "from serpapi import GoogleSearch\n",
        "\n",
        "current_date = datetime.datetime.now(\n",
        "        pytz.timezone(\"America/Los_Angeles\")\n",
        "    ).strftime(\"%B %d, %Y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "98YRRHnz0SGu"
      },
      "outputs": [],
      "source": [
        "#@title API keys\n",
        "\n",
        "# OpenAI's API key (sign up at https://platform.openai.com/signup to get $5 in\n",
        "# free credit that can be used during your first 3 months)\n",
        "openai_api_key = \"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\"  # @param {type:\"string\"}\n",
        "openai_client = OpenAI(\n",
        "  api_key=openai_api_key,\n",
        ")\n",
        "\n",
        "# SerpApi's API key (sign up at https://serpapi.com/users/sign_up?plan=free for\n",
        "# a free plan with 100 searches/month)\n",
        "serpapi_api_key = \"0a2038fc3df90982e35387b251d6ddfe47535e3eacc7e7d92b72c8dde8b4929a\"  # @param {type:\"string\"}\n",
        "\n",
        "assert openai_api_key is not None and openai_api_key != \"\", \"OpenAI's API key is not set\"\n",
        "assert serpapi_api_key is not None and serpapi_api_key != \"\", \"SerpApi's API key is not set\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7x4S8-FHHtK1"
      },
      "outputs": [],
      "source": [
        "#@title Function calling for the base LLM\n",
        "\n",
        "\n",
        "def call_llm_api(prompt, model, temperature, max_tokens, chat_completions=True):\n",
        "  # See https://platform.openai.com/docs/guides/gpt for details\n",
        "  if chat_completions:\n",
        "    # Chat completions API\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are a helpful assistant. Answer as concisely as\"\n",
        "                    f\" possible. Knowledge cutoff: {current_date}.\"\n",
        "                ),\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"What's today's date?\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": f\"Today is {current_date} in Pacific Standard Time.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    print(prompt)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  else:\n",
        "    # Completions API\n",
        "    response = openai_client.completions.create(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        prompt=prompt,\n",
        "    )\n",
        "    return response.choices[0].text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1wZfMlvj0i-Z"
      },
      "outputs": [],
      "source": [
        "#@title Function calling for the search engine\n",
        "\n",
        "\n",
        "def call_search_engine(query):\n",
        "  params = {\n",
        "    \"q\": query,\n",
        "    # \"location\": \"California, United States\",\n",
        "    \"hl\": \"en\",\n",
        "    \"gl\": \"us\",\n",
        "    \"google_domain\": \"google.com\",\n",
        "    \"api_key\": serpapi_api_key,\n",
        "\n",
        "  }\n",
        "\n",
        "  search = GoogleSearch(params)\n",
        "  return search.get_dict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oQTRnGgcxC8h"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions for FreshPrompt\n",
        "\n",
        "def is_date(string, fuzzy=False):\n",
        "  # Parse a string into a date and check its validity\n",
        "  try:\n",
        "      dateutil.parser.parse(string, fuzzy=fuzzy)\n",
        "      return True\n",
        "  except ValueError:\n",
        "      return False\n",
        "\n",
        "\n",
        "def format_date(d):\n",
        "  # Standardize the date format for each search result\n",
        "  date = dateutil.parser.parse(current_date, fuzzy=True).strftime(\"%b %d, %Y\")\n",
        "  if d is None:\n",
        "    return None\n",
        "\n",
        "  for t in [\"second\", \"minute\", \"hour\"]:\n",
        "    if f\"{t} ago\" in d or f\"{t}s ago\" in d:\n",
        "      return date\n",
        "\n",
        "  t = \"day\"\n",
        "  if f\"{t} ago\" in d or f\"{t}s ago\" in d:\n",
        "    n_days = int(re.search(\"(\\d+) days? ago\", d).group(1))\n",
        "    return (\n",
        "        datetime.datetime.strptime(date, \"%b %d, %Y\")\n",
        "        - datetime.timedelta(days=n_days)\n",
        "    ).strftime(\"%b %d, %Y\")\n",
        "\n",
        "  try:\n",
        "    return dateutil.parser.parse(d, fuzzy=True).strftime(\"%b %d, %Y\")\n",
        "  except ValueError:\n",
        "    for x in d.split():\n",
        "      if is_date(x):\n",
        "        return dateutil.parser.parse(x, fuzzy=True).strftime(\"%b %d, %Y\")\n",
        "\n",
        "\n",
        "def extract_source_webpage(link):\n",
        "  # Extract source webpage\n",
        "  return (\n",
        "      link.strip()\n",
        "      .replace(\"https://www.\", \"\")\n",
        "      .replace(\"http://www.\", \"\")\n",
        "      .replace(\"https://\", \"\")\n",
        "      .replace(\"http://\", \"\")\n",
        "      .split(\"/\")[0]\n",
        "  )\n",
        "\n",
        "\n",
        "def simplify_displayed_link(displayed_link):\n",
        "  # Simplify displayed link\n",
        "  if displayed_link is None:\n",
        "    return None\n",
        "  return extract_source_webpage(displayed_link.split(' › ')[0])\n",
        "\n",
        "\n",
        "def format_search_results(search_data, title_field=None, highlight_field=None):\n",
        "  # Standardize search results as shown in Figure 3 (left) in the paper\n",
        "  field = 'snippet_highlighted_words'\n",
        "  if field in search_data and isinstance(search_data[field], list):\n",
        "    search_data[field] = ' | '.join(search_data[field])\n",
        "\n",
        "  field = 'displayed_link'\n",
        "  if field in search_data:\n",
        "    search_data[field] = simplify_displayed_link(search_data[field])\n",
        "\n",
        "  # edge case 1\n",
        "  if search_data.get('type') == 'local_time':\n",
        "    source = search_data.get('displayed_link')\n",
        "    date = format_date(search_data.get('date'))\n",
        "    title = search_data.get('title')\n",
        "\n",
        "    snippet = search_data.get('snippet')\n",
        "    if snippet is None and 'result' in search_data:\n",
        "      if 'extensions' in search_data and isinstance(\n",
        "          search_data['extensions'], list\n",
        "      ):\n",
        "        snippet = '\\n\\t'.join(\n",
        "            [search_data['result']] + search_data['extensions']\n",
        "        )\n",
        "      else:\n",
        "        snippet = search_data['result']\n",
        "\n",
        "    highlight = search_data.get('snippet_highlighted_words')\n",
        "    if highlight is None and 'result' in search_data:\n",
        "      highlight = search_data['result']\n",
        "\n",
        "  # edge case 2\n",
        "  elif 'type' in search_data and search_data['type'] == 'population_result':\n",
        "    source = search_data.get('displayed_link')\n",
        "    if source is None and 'sources' in search_data:\n",
        "      if (\n",
        "          isinstance(search_data['sources'], list)\n",
        "          and 'link' in search_data['sources'][0]\n",
        "      ):\n",
        "        source = extract_source_webpage(search_data['sources'][0]['link'])\n",
        "\n",
        "    date = format_date(search_data.get('date'))\n",
        "    if date is None and 'year' in search_data:\n",
        "      date = format_date(search_data['year'])\n",
        "\n",
        "    title = search_data.get('title')\n",
        "\n",
        "    snippet = search_data.get('snippet')\n",
        "    if snippet is None and 'population' in search_data:\n",
        "      if 'place' in search_data:\n",
        "        snippet = '\\n\\t'.join(\n",
        "            [\n",
        "                f\"{search_data['place']} / Population\",\n",
        "            ]\n",
        "            + [\n",
        "                search_data['population'],\n",
        "            ]\n",
        "        )\n",
        "      else:\n",
        "        snippet = search_data['population']\n",
        "\n",
        "    highlight = search_data.get('snippet_highlighted_words')\n",
        "    if highlight is None and 'population' in search_data:\n",
        "      highlight = search_data['population']\n",
        "\n",
        "  else:\n",
        "    source = search_data.get('displayed_link')\n",
        "    date = format_date(search_data.get('date'))\n",
        "    title = (\n",
        "        search_data.get('title')\n",
        "        if title_field is None\n",
        "        else search_data.get(title_field)\n",
        "    )\n",
        "    highlight = (\n",
        "        search_data.get('snippet_highlighted_words')\n",
        "        if highlight_field is None\n",
        "        else search_data.get(highlight_field)\n",
        "    )\n",
        "    snippet = search_data.get('snippet', '')\n",
        "\n",
        "    if 'rich_snippet' in search_data:\n",
        "      for key in ['top', 'bottom']:\n",
        "        if (\n",
        "            key in search_data['rich_snippet']\n",
        "            and 'extensions' in search_data['rich_snippet'][key]\n",
        "        ):\n",
        "          snippet = '\\n\\t'.join(\n",
        "              [snippet] + search_data['rich_snippet'][key]['extensions']\n",
        "          )\n",
        "\n",
        "    if 'list' in search_data:\n",
        "      assert isinstance(search_data['list'], list)\n",
        "      snippet = '\\n\\t'.join([snippet] + search_data['list'])\n",
        "\n",
        "    if 'contents' in search_data and 'table' in search_data['contents']:\n",
        "      tbl = search_data['contents']['table']\n",
        "      assert isinstance(tbl, list)\n",
        "      snippet += '\\n'\n",
        "      for row in tbl:\n",
        "        snippet += f'\\n{\",\".join(row)}'\n",
        "\n",
        "    if snippet is not None and snippet.strip() == '':\n",
        "      snippet = None\n",
        "\n",
        "  return {\n",
        "      'source': source,\n",
        "      'date': date,\n",
        "      'title': title,\n",
        "      'snippet': snippet,\n",
        "      'highlight': highlight,\n",
        "  }\n",
        "\n",
        "\n",
        "def format_knowledge_graph(search_data):\n",
        "  # Standardize knowledge graphs as shown in Figure 3 (left) in the paper\n",
        "  source = None\n",
        "  if \"source\" in search_data and \"link\" in search_data[\"source\"]:\n",
        "    source = extract_source_webpage(search_data[\"source\"][\"link\"])\n",
        "\n",
        "  date = None\n",
        "\n",
        "  title = None\n",
        "  if \"title\" in search_data:\n",
        "    title = search_data[\"title\"]\n",
        "    if \"type\" in search_data:\n",
        "      title += f\"\\n\\t{search_data['type']}\"\n",
        "\n",
        "  snippet = \"\"\n",
        "  for field in search_data:\n",
        "    if (\n",
        "        (field not in [\"title\", \"type\", \"kgmid\"])\n",
        "        and (\"_link\" not in field)\n",
        "        and (\"_stick\" not in field)\n",
        "        and isinstance(search_data[field], str)\n",
        "        and not search_data[field].startswith(\"http\")\n",
        "    ):\n",
        "      snippet += f\"\\n\\t{field}: {search_data[field]}\"\n",
        "\n",
        "  if snippet.strip() == \"\":\n",
        "    snippet = None\n",
        "  else:\n",
        "    snippet = snippet.strip()\n",
        "\n",
        "  highlight = None\n",
        "\n",
        "  return {\n",
        "      \"source\": source,\n",
        "      \"date\": date,\n",
        "      \"title\": title,\n",
        "      \"snippet\": snippet,\n",
        "      \"highlight\": highlight,\n",
        "  }\n",
        "\n",
        "\n",
        "def format_questions_and_answers(search_data):\n",
        "  # Standardize questions and answers as shown in Figure 3 (left) in the paper\n",
        "  source = None\n",
        "  if \"link\" in search_data:\n",
        "    source = extract_source_webpage(search_data[\"link\"])\n",
        "\n",
        "  date = None\n",
        "\n",
        "  title = None\n",
        "  if \"question\" in search_data:\n",
        "    title = search_data[\"question\"]\n",
        "\n",
        "  snippet = None\n",
        "  if \"answer\" in search_data:\n",
        "    snippet = search_data[\"answer\"]\n",
        "\n",
        "  highlight = None\n",
        "\n",
        "  return {\n",
        "      \"source\": source,\n",
        "      \"date\": date,\n",
        "      \"title\": title,\n",
        "      \"snippet\": snippet,\n",
        "      \"highlight\": highlight,\n",
        "  }\n",
        "\n",
        "\n",
        "def freshprompt_format(\n",
        "    question,\n",
        "    search_data,\n",
        "    reasoning_and_answer,\n",
        "    num_organic_results,\n",
        "    num_related_questions,\n",
        "    num_questions_and_answers,\n",
        "    num_retrieved_evidences,\n",
        "):\n",
        "  \"\"\"Build FreshPrompt for each question\n",
        "\n",
        "  Args:\n",
        "    question: The question to process.\n",
        "    search_data: Search data.\n",
        "    reasoning_and_answer: The reasoning and answer.\n",
        "    num_organic_results: Number of organic results to keep.\n",
        "    num_related_questions: Number of related questions to keep.\n",
        "    num_questions_and_answers: Number of questions and answers to keep.\n",
        "    num_retrieved_evidences: Number of retrieved evidences to keep.\n",
        "\n",
        "  Returns:\n",
        "    A prompt that incorporates retrieved evidences for each question.\n",
        "  \"\"\"\n",
        "\n",
        "  df = pd.DataFrame(columns=['source', 'date', 'title', 'snippet', 'highlight'])\n",
        "\n",
        "  # Organic results\n",
        "  organic_results = [None] * num_organic_results\n",
        "  for k in range(num_organic_results):\n",
        "    if (\n",
        "        'organic_results' in search_data\n",
        "        and len(search_data['organic_results']) > k\n",
        "    ):\n",
        "      organic_results[k] = format_search_results(\n",
        "          search_data['organic_results'][k]\n",
        "      )\n",
        "    else:\n",
        "      organic_results[k] = format_search_results({})\n",
        "\n",
        "  for d in organic_results[::-1]:\n",
        "    df = pd.concat([df, pd.DataFrame([d])], ignore_index=True)\n",
        "\n",
        "  # Related questions\n",
        "  related_questions = [None] * num_related_questions\n",
        "  for k in range(num_related_questions):\n",
        "    if (\n",
        "        'related_questions' in search_data\n",
        "        and len(search_data['related_questions']) > k\n",
        "    ):\n",
        "      related_questions[k] = format_search_results(\n",
        "          search_data['related_questions'][k], title_field='question'\n",
        "      )\n",
        "    else:\n",
        "      related_questions[k] = format_search_results({})\n",
        "\n",
        "  for d in related_questions[::-1]:\n",
        "    df = pd.concat([df, pd.DataFrame([d])], ignore_index=True)\n",
        "\n",
        "  # Questions and Answers\n",
        "  questions_and_answers = [None] * num_questions_and_answers\n",
        "  for k in range(num_questions_and_answers):\n",
        "    if (\n",
        "        'questions_and_answers' in search_data\n",
        "        and len(search_data['questions_and_answers']) > k\n",
        "    ):\n",
        "      questions_and_answers[k] = format_questions_and_answers(\n",
        "          search_data['questions_and_answers'][k]\n",
        "      )\n",
        "    else:\n",
        "      questions_and_answers[k] = format_questions_and_answers({})\n",
        "\n",
        "  for d in questions_and_answers[::-1]:\n",
        "    df = pd.concat([df, pd.DataFrame([d])], ignore_index=True)\n",
        "\n",
        "  # Knowledge graph\n",
        "  knowledge_graph = None\n",
        "  if 'knowledge_graph' in search_data:\n",
        "    knowledge_graph = format_knowledge_graph(search_data['knowledge_graph'])\n",
        "  else:\n",
        "    knowledge_graph = format_knowledge_graph({})\n",
        "  df = pd.concat([df, pd.DataFrame([knowledge_graph])], ignore_index=True)\n",
        "\n",
        "  # Answer box\n",
        "  answer_box = None\n",
        "  if 'answer_box' in search_data:\n",
        "    answer_box = format_search_results(\n",
        "        search_data['answer_box'], highlight_field='answer'\n",
        "    )\n",
        "  else:\n",
        "    answer_box = format_search_results({})\n",
        "  df = pd.concat([df, pd.DataFrame([answer_box])], ignore_index=True)\n",
        "\n",
        "  # Sort by date\n",
        "  df['date'] = df['date'].apply(lambda x: format_date(x))\n",
        "  df['datetime'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "  df = df.sort_values(by='datetime', na_position='first')\n",
        "  df.replace({pd.NaT: None}, inplace=True)\n",
        "  df = df.dropna(how='all')\n",
        "\n",
        "  # Select top_k supporting evidences overall\n",
        "  evidences = []\n",
        "  for _, row in df.tail(num_retrieved_evidences).iterrows():\n",
        "    evidences.append(\n",
        "        f\"\"\"\\n\\nsource: {row['source']}\\ndate: {row['date']}\\ntitle: {row['title']}\\nsnippet: {row['snippet']}\\nhighlight: {row['highlight']}\"\"\"\n",
        "    )\n",
        "  #for i in evidences:\n",
        "   # print(i)\n",
        "  return (\n",
        "      ''.join(\n",
        "          [\n",
        "              f'\\n\\n\\nquery: {question}',\n",
        "          ]\n",
        "          + evidences\n",
        "      )\n",
        "      + f'\\n\\nquestion: {question}{reasoning_and_answer}'\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmQZYfPD3sxL"
      },
      "outputs": [],
      "source": [
        "#@title Demonstration examples\n",
        "\n",
        "\n",
        "demo_questions = [\n",
        "    \"What year is considered Albert Einstein's annus mirabilis?\",\n",
        "    \"Which photographer took the most expensive photograph in the world?\",\n",
        "    \"How many days are left until the 2023 Grammy Awards?\",\n",
        "    \"How many years ago did the Boxing Day Tsunami happen?\",\n",
        "    (\n",
        "        \"When did Amazon become the first publicly traded company to exceed a\"\n",
        "        \" market value of $3 trillion?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "concise_demo_reasonings_and_answers = [\n",
        "    (\n",
        "        \"1905 is considered Albert Einstein's annus mirabilis, his miraculous\"\n",
        "        \" year.\"\n",
        "    ),\n",
        "    (\n",
        "        'The most expensive photograph in the world is \"Le Violon d\\'Ingres\".'\n",
        "        \" The photograph was created by Man Ray.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The 2023 Grammy Awards ceremony was held on February 5, 2023. Thus,\"\n",
        "        \" the ceremony has already taken place.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The disaster occurred on December 26, 2004. Thus, it happened 18 years\"\n",
        "        \" ago.\"\n",
        "    ),\n",
        "    \"Amazon's market capitalization has never exceeded $3 trillion.\",\n",
        "]\n",
        "\n",
        "verbose_demo_reasonings_and_answers = [\n",
        "    (\n",
        "        \"In the year of 1905, Albert Einstein published four groundbreaking\"\n",
        "        \" papers that revolutionized scientific understanding of the universe.\"\n",
        "        \" Thus, scientists call 1905 Albert Einstein's annus mirabilis — his\"\n",
        "        \" year of miracles.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Man Ray's famed \\\"Le Violon d'Ingres\\\" became the most expensive\"\n",
        "        \" photograph ever to sell at auction, sold for $12.4 million on May\"\n",
        "        \" 14th, 2022 at Christie's New York. The black and white image, taken\"\n",
        "        \" in 1924 by the American surrealist artist, transforms a woman's naked\"\n",
        "        \" body into a violin by overlaying the picture of her back with\"\n",
        "        \" f-holes. Thus, Man Ray is the photographer who took the most\"\n",
        "        \" expensive photograph in the world.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The 2023 Grammy Awards, officially known as the 65th Annual Grammy\"\n",
        "        \" Awards ceremony, was held in Los Angeles on February 5, 2023. Thus,\"\n",
        "        \" the event has already taken place.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The Boxing Day Tsunami refers to the 2004 Indian Ocean earthquake and\"\n",
        "        \" tsunami, which is one of the deadliest natural disasters in recorded\"\n",
        "        \" history, killing an estimated 230,000 people across 14 countries. The\"\n",
        "        \" disaster occurred on December 26, 2004, which is 18 years ago.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Amazon's market capitalization hit a peak of roughly $1.9 trillion in\"\n",
        "        \" July 2021. In 2022, Amazon became the first public company ever to\"\n",
        "        \" lose $1 trillion in market value. Thus, Amazon's market value has\"\n",
        "        \" never exceeded $3 trillion. In fact, Apple became the first publicly\"\n",
        "        \" traded U.S. company to exceed a market value of $3 trillion in\"\n",
        "        \" January 2022.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "prefix = (\n",
        "    f\"\\nanswer: As of today {current_date}, the most up-to-date and relevant\"\n",
        "    \" information regarding this query is as follows. \"\n",
        ")\n",
        "\n",
        "concise_demo_reasonings_and_answers = [\n",
        "    prefix + x for x in concise_demo_reasonings_and_answers\n",
        "]\n",
        "verbose_demo_reasonings_and_answers = [\n",
        "    prefix + x for x in verbose_demo_reasonings_and_answers\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS6fKTK-A8bY"
      },
      "outputs": [],
      "source": [
        "#@title Retrieving search data for demonstration examples\n",
        "\n",
        "demo_search_data = [call_search_engine(q) for q in demo_questions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAcuImw5EP5T"
      },
      "outputs": [],
      "source": [
        "#@title Function calling for FreshPrompt\n",
        "\n",
        "\n",
        "def call_freshprompt(model, question, check_premise=False, verbose=False):\n",
        "  temperature = 0.0\n",
        "  max_tokens = 256\n",
        "  chat_completions = True\n",
        "\n",
        "  if model.startswith('gpt-4'):\n",
        "    num_organic_results = 15\n",
        "    num_related_questions = 3\n",
        "    num_questions_and_answers = 3\n",
        "    num_retrieved_evidences = 15\n",
        "  else:\n",
        "    num_organic_results = 15\n",
        "    num_related_questions = 2\n",
        "    num_questions_and_answers = 2\n",
        "    num_retrieved_evidences = 5\n",
        "\n",
        "  if verbose:\n",
        "    demo_reasonings_and_answers = verbose_demo_reasonings_and_answers\n",
        "  else:\n",
        "    demo_reasonings_and_answers = concise_demo_reasonings_and_answers\n",
        "\n",
        "  # Generate prompts for demo examples\n",
        "  demo_prompts = []\n",
        "  for q, s, ra in zip(\n",
        "      demo_questions, demo_search_data, concise_demo_reasonings_and_answers\n",
        "  ):\n",
        "      demo_prompts.append(\n",
        "      freshprompt_format(\n",
        "          q,\n",
        "          s,\n",
        "          ra,\n",
        "          num_organic_results,\n",
        "          num_related_questions,\n",
        "          num_questions_and_answers,\n",
        "          num_retrieved_evidences,\n",
        "      )\n",
        "      )\n",
        "\n",
        "  freshprompt_demo = ''.join(demo_prompts).strip()\n",
        "\n",
        "  if check_premise:\n",
        "    suffix = (\n",
        "        \"\\nPlease check if the question contains a valid premise before\"\n",
        "        \" answering.\\nanswer: \"\n",
        "    )\n",
        "  else:\n",
        "    suffix = \"\\nanswer: \"\n",
        "  freshprompt_question = freshprompt_format(\n",
        "      question,\n",
        "      call_search_engine(question),\n",
        "      suffix,\n",
        "      num_organic_results,\n",
        "      num_related_questions,\n",
        "      num_questions_and_answers,\n",
        "      num_retrieved_evidences,\n",
        "  )\n",
        "  fresh_prompt = freshprompt_demo + freshprompt_question\n",
        "  answer = call_llm_api(\n",
        "      fresh_prompt, model, temperature, max_tokens, chat_completions\n",
        "  )\n",
        "  return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfeN7Mt1kvvT"
      },
      "outputs": [],
      "source": [
        "#what is the name of the nearest concert BlackPink in VietNam?\n",
        "#Which team is at the top of the Premier League right now?\n",
        "#What is the name of the first animal to land on the moon?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrmUKnXU1PPB",
        "outputId": "9d54fd7d-73f5-4ddf-a735-ab228f087c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "query: What year is considered Albert Einstein's annus mirabilis?\n",
            "\n",
            "source: quora.com\n",
            "date: Jul 26, 2016\n",
            "title: What caused Einstein's annus mirabilis?\n",
            "snippet: Were the year 1905 and the papers Einstein published that year so “miraculous” that they cannot be explained reasonably or rationally ?\n",
            "highlight: 1905\n",
            "\n",
            "source: hsm.stackexchange.com\n",
            "date: Apr 11, 2017\n",
            "title: How did the publication feat of Einstein's four 1905 Annus ...\n",
            "snippet: Yet in 1905, Einstein's \"Annus Mirabilis,\" he published 4 separate ... What was Einstein's first publication of the final form of general ...\n",
            "highlight: 1905\n",
            "\n",
            "source: scihi.org\n",
            "date: Jun 30, 2018\n",
            "title: The Annus Mirabilis in Physics – Albert Einstein and the Year ...\n",
            "snippet: 1905 was this Annus Mirabilis, this year of wonders or extraordinary year. History considers 1905 as the year with the most outstanding and ...\n",
            "highlight: 1905\n",
            "\n",
            "source: guides.loc.gov\n",
            "date: Nov 06, 2019\n",
            "title: Introduction - Annus Mirabilis of Albert Einstein\n",
            "snippet: He published hundreds of scientific papers during the course of his life, but in 1905, the year he turned 26, he published the four ...\n",
            "highlight: 1905\n",
            "\n",
            "source: guides.loc.gov\n",
            "date: Nov 06, 2019\n",
            "title: When was the annus mirabilis of Albert Einstein?\n",
            "snippet: In 1905 Albert Einstein published four groundbreaking papers that revolutionized scientific understanding of the universe. This is a guide to resources on the Annus Mirabilis of Albert Einstein.\n",
            "highlight: None\n",
            "\n",
            "question: What year is considered Albert Einstein's annus mirabilis?\n",
            "answer: As of today December 11, 2023, the most up-to-date and relevant information regarding this query is as follows. 1905 is considered Albert Einstein's annus mirabilis, his miraculous year.\n",
            "\n",
            "\n",
            "query: Which photographer took the most expensive photograph in the world?\n",
            "\n",
            "source: pocket-lint.com\n",
            "date: Jul 22, 2021\n",
            "title: The 24 most expensive photos that sold for millions\n",
            "snippet: Australian photographer Peter Lik claims to hold the record for the most expensive photograph ever sold: a staggering $6.5 million for his photo ...\n",
            "highlight: Australian photographer Peter Lik\n",
            "\n",
            "source: cnn.com\n",
            "date: May 15, 2022\n",
            "title: What photo was sold for millions?\n",
            "snippet: The current record for a photograph sold at auction is held by Andreas Gursky's Rhine II, which was sold by Christie's for a staggering $4.3 million in 2011.\n",
            "highlight: None\n",
            "\n",
            "source: all-about-photo.com\n",
            "date: Dec 20, 2022\n",
            "title: What is the world's most expensive photograph?\n",
            "snippet: The most expensive image ever sold at auction, Le Violon d'Ingres (1924) by Man Ray, which features a nude woman's back superimposed with a violin's f-holes, sold for $12.4 million on May 14th, 2022 at Christie's New York.\n",
            "highlight: None\n",
            "\n",
            "source: artincontext.org\n",
            "date: Aug 10, 2023\n",
            "title: Most Expensive Photographs - 15 Extravagant Photographs\n",
            "snippet: Le Violon D'Ingre (1924) by Man Ray takes first place at an astonishing $12.4 million! Some of the other most expensive photos ever sold include ...\n",
            "highlight: Le Violon D'Ingre (1924) by Man Ray\n",
            "\n",
            "source: artisanhd.com\n",
            "date: Oct 25, 2023\n",
            "title: Top 5 Most Expensive Photographs & Why We Love Them\n",
            "snippet: Our Top 5 Favorite Most Expensive Photographs · Andreas Gursky: Rhein II (1999) – $4.3M · Edward Steichen: The Flatiron – $11.8M · Richard Prince: ...\n",
            "\t‎Free 3–9 day delivery\n",
            "\t‎10\n",
            "\tday returns\n",
            "highlight: Andreas Gursky: Rhein II\n",
            "\n",
            "question: Which photographer took the most expensive photograph in the world?\n",
            "answer: As of today December 11, 2023, the most up-to-date and relevant information regarding this query is as follows. The most expensive photograph in the world is \"Le Violon d'Ingres\". The photograph was created by Man Ray.\n",
            "\n",
            "\n",
            "query: How many days are left until the 2023 Grammy Awards?\n",
            "\n",
            "source: billboard.com\n",
            "date: Jul 14, 2022\n",
            "title: 2023 Grammy Awards Date & Location Revealed\n",
            "snippet: Here's the Date & Location of the 2023 Grammy Awards. Nominations will be announced on Tuesday, Nov. 15 -- eight days earlier than they were ...\n",
            "highlight: 15 -- eight days\n",
            "\n",
            "source: eonline.com\n",
            "date: Nov 15, 2022\n",
            "title: How to Watch the 2023 Grammy Nominations\n",
            "snippet: The nominations for the 2023 Grammy Awards will be announced on Nov. 15, just two days before 23rd Annual Latin Grammy Awards ceremony.\n",
            "highlight: two days\n",
            "\n",
            "source: billboard.com\n",
            "date: Dec 14, 2022\n",
            "title: Final Round Voting for 2023 Grammy Awards Has Begun\n",
            "snippet: 2023 Grammy Awards final round voting opened on Wednesday (Dec. 14) and continues until Jan. 4, 2023.\n",
            "highlight: Jan. 4, 2023\n",
            "\n",
            "source: en.as.com\n",
            "date: Feb 01, 2023\n",
            "title: 2023 Grammy Awards: Who's attending, skipping ... - AS USA\n",
            "snippet: With only a few days left until the 65th annual Grammy Awards, and the full performer lineup yet to be announced, here is how the show is ...\n",
            "highlight: only a few days\n",
            "\n",
            "source: howmanydaysuntil.center\n",
            "date: Feb 04, 2023\n",
            "title: When ar The Grammy Awards - How Many Days Until\n",
            "snippet: The Grammy Awards will be celebrated this year on is on February 4th. Find out more about The Grammy Awards.\n",
            "highlight: February 4th\n",
            "\n",
            "question: How many days are left until the 2023 Grammy Awards?\n",
            "answer: As of today December 11, 2023, the most up-to-date and relevant information regarding this query is as follows. The 2023 Grammy Awards ceremony was held on February 5, 2023. Thus, the ceremony has already taken place.\n",
            "\n",
            "\n",
            "query: How many years ago did the Boxing Day Tsunami happen?\n",
            "\n",
            "source: materchristi.libguides.com\n",
            "date: May 11, 2023\n",
            "title: 2004 Indian Ocean Tsunami - Natural disasters of the Asia ...\n",
            "snippet: The Boxing Day tsunami in 2004 is believed to be the deadliest tsunami in history, killing more than 230,000 people across 14 countries.\n",
            "highlight: 2004\n",
            "\n",
            "source: noaa.gov\n",
            "date: Jun 12, 2023\n",
            "title: JetStream Max: 2004 Indian Ocean Tsunami\n",
            "snippet: On December 26, 2004, an extremely powerful earthquake off the coast of the Indonesian island of Sumatra in the Indian Ocean generated a ...\n",
            "highlight: December 26, 2004\n",
            "\n",
            "source: noaa.gov\n",
            "date: Jun 12, 2023\n",
            "title: When did the Boxing Day tsunami occur?\n",
            "snippet: On December 26, 2004, an extremely powerful earthquake off the coast of the Indonesian island of Sumatra in the Indian Ocean generated a devastating tsunami.\n",
            "highlight: None\n",
            "\n",
            "source: worldvision.org\n",
            "date: Sep 25, 2023\n",
            "title: 2004 Indian Ocean Earthquake and Tsunami\n",
            "snippet: The earthquake struck 150 miles from the coast of Sumatra Island, on the northwest of the Indonesian island group, and 31 miles below the ocean ...\n",
            "highlight: earthquake | Indonesian\n",
            "\n",
            "source: worldvision.org\n",
            "date: Sep 25, 2023\n",
            "title: What was the worst tsunami in 2004?\n",
            "snippet: Approximately 230,000 people died in the 2004 Indian Ocean earthquake and tsunami, making it one of the deadliest disasters in modern history. The Sumatra-Andaman earthquake, which caused the tsunami, is estimated to have released energy equivalent to 23,000 Hiroshima-type atomic bombs.\n",
            "highlight: None\n",
            "\n",
            "question: How many years ago did the Boxing Day Tsunami happen?\n",
            "answer: As of today December 11, 2023, the most up-to-date and relevant information regarding this query is as follows. The disaster occurred on December 26, 2004. Thus, it happened 18 years ago.\n",
            "\n",
            "\n",
            "query: When did Amazon become the first publicly traded company to exceed a market value of $3 trillion?\n",
            "\n",
            "source: bloomberg.com\n",
            "date: Jun 30, 2023\n",
            "title: Apple (AAPL) Stock Hits $3 Trillion Value, First Ever\n",
            "snippet: Apple first became the world's most valuable stock in 2011, when its market cap was under $340 billion and it comprised about 3.3% of the S&P ...\n",
            "highlight: 2011\n",
            "\n",
            "source: qz.com\n",
            "date: Jun 30, 2023\n",
            "title: Apple is officially the world's first-ever $3 trillion company\n",
            "snippet: Apple became the first company in the world with a $3 trillion valuation on Friday (June 30). The company's stock closed at $193.97 per share— ...\n",
            "highlight: Friday (June 30\n",
            "\n",
            "source: nbcnews.com\n",
            "date: Jun 30, 2023\n",
            "title: Is Apple the first company to hit $3 trillion market value?\n",
            "snippet: The news shows investors remain bullish on the stock and Apple's portfolio of products and services, despite the company's warning in May that its current quarter revenue is expected to fall about 3%.\n",
            "highlight: None\n",
            "\n",
            "source: fortune.com\n",
            "date: Jun 30, 2023\n",
            "title: Which company was the first to reach a stock market value of $3 trillion?\n",
            "snippet: Apple just made history by becoming the first company with a $3 trillion market value—'and its lock on the consumer is only getting stronger'\n",
            "highlight: None\n",
            "\n",
            "source: money.usnews.com\n",
            "date: Oct 24, 2023\n",
            "title: The 10 Most Valuable Companies in the World by Market Cap\n",
            "snippet: (ticker: AAPL) was the first company to reach a market capitalization of $1 trillion in 2018. ... Overall, Amazon has become the largest online ...\n",
            "highlight: 2018\n",
            "\n",
            "question: When did Amazon become the first publicly traded company to exceed a market value of $3 trillion?\n",
            "answer: As of today December 11, 2023, the most up-to-date and relevant information regarding this query is as follows. Amazon's market capitalization has never exceeded $3 trillion.\n",
            "\n",
            "\n",
            "query: What is the name of the first animal to land on the moon?\n",
            "\n",
            "source: theatlantic.com\n",
            "date: Dec 27, 2012\n",
            "title: Who Was First in the Race to the Moon? The Tortoise\n",
            "snippet: None\n",
            "highlight: None\n",
            "\n",
            "source: space.stackexchange.com\n",
            "date: Dec 28, 2013\n",
            "title: What was the first animal sent into space?\n",
            "snippet: The first animals sent into space were fruit flies aboard a U.S.-launched V-2 rocket on February 20, 1947. The purpose of the experiment was to ...\n",
            "highlight: fruit flies\n",
            "\n",
            "source: quora.com\n",
            "date: Apr 16, 2019\n",
            "title: Who was the first animal to go on moon?\n",
            "snippet: A dog by the name of Laika was the first animal to go to the moon. On November 3, 1957, Laika, a stray dog from Moscow, Russia, was sent ...\n",
            "highlight: Laika\n",
            "\n",
            "source: cnn.com\n",
            "date: Jan 16, 2020\n",
            "title: What was the first animal to come on land?\n",
            "snippet: A 437-million-year old scorpion was the earliest known creature to venture from sea onto land, a new study has found, shedding new light on one of the earliest chapters in the planet's evolutionary history.\n",
            "highlight: None\n",
            "\n",
            "source: space.com\n",
            "date: Jan 27, 2022\n",
            "title: Animals in space\n",
            "snippet: The first mammal in space was Albert II, a rhesus monkey launched by NASA who reached an altitude of 83 miles (134 km) on June 14, 1949. Albert ...\n",
            "highlight: Albert II\n",
            "\n",
            "question: What is the name of the first animal to land on the moon?\n",
            "Please check if the question contains a valid premise before answering.\n",
            "answer: \n",
            "The premise of the question is not valid. No animals have landed on the moon. The first living beings to travel to space were fruit flies, followed by various other animals, but no animals have landed on the moon.\n"
          ]
        }
      ],
      "source": [
        "#@title FreshPrompt\n",
        "\n",
        "# @markdown ---\n",
        "model_name = \"gpt-3.5-turbo-1106\" #@param [\"gpt-4-1106-preview\", \"gpt-4\", \"gpt-4-32k\", \"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo\"]\n",
        "check_premise = True  # @param {type:\"boolean\"}\n",
        "# @markdown ### Ask your question here!\n",
        "\n",
        "question = \"What is the name of the first animal to land on the moon?\"  # @param {type:\"string\"}\n",
        "answer = call_freshprompt(model_name, question, check_premise=check_premise)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpXYWD4IxGDk"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}